# Ilya Sutskever的30篇Deep learning必读论文
> Ilya Sutskever给了John Carmack一份包含大约30篇研究论文的阅读清单，并说：“如果你真正掌握了这些内容，你就会了解当今AI相关90%的重要知识。”

慢速整理中英文对照翻译好的论文方便大家阅读


## The First Law of Complexodynamics <br>
**阅读：**[中英对照翻译版](https://volctracer.com/w/dreqeBgN)<br>


**总结：** 文章主要探讨循环神经网络（RNN），特别是在处理任意长度序列数据方面的优势。RNN通过保留隐藏状态来捕捉复杂的文本结构，适用于自然语言处理等领域。文章介绍了RNN和长短期记忆网络（LSTM）的机制，并通过字符级语言模型的训练，展示了RNN在文本生成和语法理解中的潜力，强调了其在多个领域的重要性。<br>


## The Unreasonable Effectiveness of Recurrent Neural Networks<br>
**阅读：**[中英对照翻译版](https://volctracer.com/w/faBFNTP0)<br>


**总结：** 文章主要探讨循环神经网络（RNN）。Karpathy分享了他在图像描述任务中首次训练RNN的经验，尽管使用随机设置，RNN仍能生成可信的描述，展示了其简单而强大的特性。RNN能够处理任意长度的序列输入和输出，通过保留隐藏状态来“记住”过去的数据。文章详细介绍了RNN和长短期记忆网络（LSTM）的工作原理，并通过字符级语言模型的训练展示了RNN在文本生成中的潜力。Karpathy提供了多个RNN生成文本的实例，说明其在学习复杂结构、语法和语境方面的能力。文章还讨论了RNN训练过程的改进，强调其在自然语言处理、计算机视觉和机器学习等领域的重要性。<br>


## Understanding LSTM Networks<br>
**阅读：**[中英对照翻译版](https://volctracer.com/w/5OOG019P)<br>


**总结：** 
介绍了长短期记忆网络（LSTM）的结构和功能。LSTM是一种改进的循环神经网络（RNN），通过引入细胞状态和输入、遗忘、输出三个门机制，有效解决了传统RNN在处理长期依赖性时的不足。文章详细阐述了LSTM的工作原理，并讨论了其变体如门控循环单元（GRU）。LSTM在语言建模、翻译和语音识别等领域取得了显著成果，并推动了神经网络研究的进一步发展。<br>


## Recurrent Neural Network Regularization<br>
**阅读：**[中英对照翻译版](https://volctracer.com/w/QqtAz0OX)<br>

